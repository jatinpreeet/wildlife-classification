{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xob66DEUjnu0",
        "outputId": "2ad60dd6-6e48-4601-fb13-d888dbcc2da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch==2.5.1 (from torchvision)\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->torchvision)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 torchvision-0.20.1 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch and torchvision\n",
        "!pip install torchvision --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifmynvmxloMG"
      },
      "source": [
        "## Device Agnostic Code , it checks if the gpu is available or not , and puts the device to GPU if available otherwise CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGWJUta7kHo6",
        "outputId": "9ef79828-1d76-45de-9119-123d7ed0379e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"using device {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4lAb8oYsmQW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUFa1ydop7Jr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC9BYSi-k9ZY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPqewBbakNxb"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/archive/animals'\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/data2/train'\n",
        "val_dir = '/content/drive/MyDrive/data2/val'\n",
        "\n",
        "os.makedirs(train_dir , exist_ok=True)\n",
        "os.makedirs(val_dir , exist_ok=True)\n",
        "\n",
        "train_split = 0.8\n",
        "\n",
        "for animals in os.listdir(data_dir):\n",
        "  animal_folder = os.path.join(data_dir,animals)\n",
        "\n",
        "  if os.path.isdir(animal_folder):\n",
        "\n",
        "    images = os.listdir(animal_folder)\n",
        "\n",
        "    random.shuffle(images)\n",
        "\n",
        "    split_point = int(len(images) * train_split)\n",
        "\n",
        "    train_images = images[:split_point]\n",
        "    val_images = images[split_point:]\n",
        "\n",
        "    train_animal_folder = os.path.join(train_dir,animals)\n",
        "    val_animal_folder = os.path.join(val_dir,animals)\n",
        "    os.makedirs(train_animal_folder,exist_ok=True)\n",
        "    os.makedirs(val_animal_folder,exist_ok=True)\n",
        "\n",
        "    for image in train_images:\n",
        "      src = os.path.join(animal_folder,image)\n",
        "      dst = os.path.join(train_animal_folder,image)\n",
        "      shutil.copy(src,dst)\n",
        "\n",
        "\n",
        "    for image in val_images:\n",
        "      src = os.path.join(animal_folder,image)\n",
        "      dst = os.path.join(val_animal_folder,image)\n",
        "      shutil.copy(src,dst)\n",
        "\n",
        "    print(f\"Processed {animals}: {len(train_images)} images in train, {len(val_images)} in val.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhokit-_Zf44"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIdQa5u0WR3w"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define paths for the train and validation datasets that we created above\n",
        "train_dir = '/content/drive/MyDrive/data2/train'\n",
        "val_dir = '/content/drive/MyDrive/data2/val'\n",
        "\n",
        "# Define transformations/data augmentations that we are going to apply on the pictures\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)), # resize all images to 224X224 , makes sure all images have same dimension\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(), #converts images from PIL format into pyTorch tensors\n",
        "    transforms.Normalize(mean =[0.485,0.456,0.406],\n",
        "                         std =[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                         std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir,transform = train_transform)\n",
        "\n",
        "val_dataset = datasets.ImageFolder(val_dir,transform = val_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrbKkrTnY-Zf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size =32\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size = batch_size,num_workers =2 , pin_memory = True,shuffle = True) # num_workers =2 , means more process to fetch data and faster\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size,num_workers = 2 , pin_memory = True, shuffle= False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdNky5m_ZHu1",
        "outputId": "88f2f31e-ec21-4152-894c-e914208847f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7ccc8ed4d690>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUhMOH1d0NtY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty5AUyXA6Fx3",
        "outputId": "a300e622-35c0-47b9-9b7d-1b25e21a1f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:05<00:00, 65.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained ViT-B-16 model\n",
        "\n",
        "weights = ViT_B_16_Weights.DEFAULT\n",
        "model = vit_b_16(weights = weights)\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "#Replace the classification head\n",
        "# - model.heads.head is the final classification layer.\n",
        "# - We replace it with a new nn.Linear layer.\n",
        "# - in_features is the size of the input to the head (768 for vit_b_16).\n",
        "# - out_features is the number of classes in your dataset.\n",
        "model.heads.head =  nn.Linear(in_features = model.heads.head.in_features, out_features = num_classes)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcDM4sAo7y-t"
      },
      "source": [
        "## Not freezing layers , so that the model can change its whole parameters a little bit to get better at our task of classification , its already good at image classification but we are making it best for wildlife images , so thats why I want to keep it like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_jDQ3JB9H5p"
      },
      "outputs": [],
      "source": [
        "# Setting up loss function and optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-5) # smaller learning rates are recommended for fine tuning , changing paramters drastically can disturb pre-trained weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_epochs = 20 # the number of times entire training dataset will be passed through model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "  print('-' * 10)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0 # 0.0 to accumualte the loss over all batches for avg loss at end of epoch\n",
        "  running_corrects = 0.0 # correct predictions #0.0 resets the accumulator for the new epoch\n",
        "\n",
        "  for inputs,labels in train_loader:\n",
        "    inputs = inputs.to(device) # batchh of input images\n",
        "    labels = labels.to(device) # true labels for input images\n",
        "\n",
        "    #zero parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    _,preds = torch.max(outputs,1) # - we are ignoring first value\n",
        "    loss = loss_fn(outputs,labels)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "\n",
        "  epoch_loss = running_loss/ len(train_dataset)\n",
        "  epoch_acc = running_corrects.double()/len(train_dataset)\n",
        "\n",
        "  print(f'Training loss:{epoch_loss:4f}Acc:{epoch_acc:.4f}')\n",
        "\n",
        "        # Validation Phase\n",
        "  model.eval()  # Set model to evaluation mode\n",
        "  val_running_loss = 0.0\n",
        "  val_running_corrects = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in val_loader:\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          val_running_loss += loss.item() * inputs.size(0)\n",
        "          val_running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  val_epoch_loss = val_running_loss / len(val_dataset)\n",
        "  val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
        "\n",
        "  print(f'Validation Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vh_qUmBY_b1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssx0BQLyTeJx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm  # For progress bars\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter  # For logging\n",
        "\n",
        "def train_model(model, train_loader, val_loader, loss_fn, optimizer,\n",
        "                num_epochs=10, device='cuda', scheduler=None):\n",
        "\n",
        "    # tnesorBoard\n",
        "    writer = SummaryWriter('runs/wildlife_classifier')\n",
        "\n",
        "    # Initialize best model tracking\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print('-' * 10)\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Progress bar for training\n",
        "        train_pbar = tqdm(train_loader, desc='Training')\n",
        "        for inputs, labels in train_pbar:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.cuda.amp.autocast():  # Mixed precision training\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        # Calculate epoch statistics\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Store in history\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc.item())\n",
        "\n",
        "        # Log to tensorboard\n",
        "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
        "\n",
        "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_running_corrects = 0\n",
        "\n",
        "        # Progress bar for validation\n",
        "        val_pbar = tqdm(val_loader, desc='Validation')\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_pbar:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                val_running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                val_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_running_corrects.double() / len(val_loader.dataset)\n",
        "\n",
        "        # Store in history\n",
        "        history['val_loss'].append(val_epoch_loss)\n",
        "        history['val_acc'].append(val_epoch_acc.item())\n",
        "\n",
        "        # Log to tensorboard\n",
        "        writer.add_scalar('Loss/val', val_epoch_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/val', val_epoch_acc, epoch)\n",
        "\n",
        "        print(f'Validation Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_epoch_acc > best_val_acc:\n",
        "            best_val_acc = val_epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            # Save checkpoint\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'history': history\n",
        "            }, 'best_model_checkpoint.pth')\n",
        "            print(\"Model improved! Checkpoint saved.\")\n",
        "\n",
        "        # Step the scheduler if provided\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_epoch_loss)\n",
        "\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f\"Epoch completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    writer.close()\n",
        "\n",
        "    return model, history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                      patience=3, factor=0.1)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model, history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=10,\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KN_qPj8ZRnH",
        "outputId": "7255ca82-576a-40e8-aa60-bcb9af70753a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/152 [00:00<?, ?it/s]<ipython-input-10-5cc2748208d8>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Mixed precision training\n",
            "Training: 100%|██████████| 152/152 [14:21<00:00,  5.66s/it, loss=0.664]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5152 Acc: 0.7238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [05:19<00:00,  5.15s/it, loss=0.0844]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3781 Acc: 0.9344\n",
            "Model improved! Checkpoint saved.\n",
            "Epoch completed in 19m 45s\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:33<00:00,  1.63it/s, loss=0.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2896 Acc: 0.9371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:36<00:00,  1.71it/s, loss=0.0241]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2039 Acc: 0.9573\n",
            "Model improved! Checkpoint saved.\n",
            "Epoch completed in 2m 15s\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:31<00:00,  1.66it/s, loss=0.323]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1178 Acc: 0.9765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:36<00:00,  1.72it/s, loss=0.0162]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1657 Acc: 0.9593\n",
            "Model improved! Checkpoint saved.\n",
            "Epoch completed in 2m 14s\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:29<00:00,  1.70it/s, loss=0.155]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1532 Acc: 0.9646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:36<00:00,  1.68it/s, loss=0.0172]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1754 Acc: 0.9588\n",
            "Epoch completed in 2m 6s\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:29<00:00,  1.69it/s, loss=0.126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0782 Acc: 0.9845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:35<00:00,  1.73it/s, loss=0.00463]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1003 Acc: 0.9771\n",
            "Model improved! Checkpoint saved.\n",
            "Epoch completed in 2m 10s\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:30<00:00,  1.69it/s, loss=0.0629]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0652 Acc: 0.9850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:36<00:00,  1.69it/s, loss=0.00618]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1675 Acc: 0.9624\n",
            "Epoch completed in 2m 7s\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:32<00:00,  1.64it/s, loss=0.203]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1372 Acc: 0.9639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:36<00:00,  1.72it/s, loss=0.00694]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1778 Acc: 0.9532\n",
            "Epoch completed in 2m 9s\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:32<00:00,  1.64it/s, loss=0.0196]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0801 Acc: 0.9798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:35<00:00,  1.76it/s, loss=0.006]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2024 Acc: 0.9522\n",
            "Epoch completed in 2m 8s\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:31<00:00,  1.65it/s, loss=0.073]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0829 Acc: 0.9794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:35<00:00,  1.74it/s, loss=0.00881]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1556 Acc: 0.9619\n",
            "Epoch completed in 2m 7s\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 152/152 [01:29<00:00,  1.69it/s, loss=0.0044]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0193 Acc: 0.9965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 62/62 [00:37<00:00,  1.66it/s, loss=0.00297]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0957 Acc: 0.9771\n",
            "Epoch completed in 2m 7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16\n",
        "\n",
        "# Initialize the model architecture\n",
        "model = vit_b_16(weights=None)  # No pre-trained weights since we'll load our own\n",
        "num_classes = len(train_dataset.classes)\n",
        "model.heads.head = nn.Linear(in_features=model.heads.head.in_features, out_features=num_classes)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load('best_model_checkpoint.pth', map_location='cpu'))\n",
        "model.eval()  # Set the model to evaluation mode"
      ],
      "metadata": {
        "id": "G_Zrc471OYvW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "bed66695-95ae-4879-af5a-9768b3126265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-69e6afe6ad07>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model_checkpoint.pth', map_location='cpu'))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"class_token\", \"conv_proj.weight\", \"conv_proj.bias\", \"encoder.pos_embedding\", \"encoder.layers.encoder_layer_0.ln_1.weight\", \"encoder.layers.encoder_layer_0.ln_1.bias\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_0.ln_2.weight\", \"encoder.layers.encoder_layer_0.ln_2.bias\", \"encoder.layers.encoder_layer_0.mlp.0.weight\", \"encoder.layers.encoder_layer_0.mlp.0.bias\", \"encoder.layers.encoder_layer_0.mlp.3.weight\", \"encoder.layers.encoder_layer_0.mlp.3.bias\", \"encoder.layers.encoder_layer_1.ln_1.weight\", \"encoder.layers.encoder_layer_1.ln_1.bias\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_1.ln_2.weight\", \"encoder.layers.encoder_layer_1.ln_2.bias\", \"encoder.layers.encoder_layer_1.mlp.0.weight\", \"encoder.layers.encoder_layer_1.mlp.0.bias\", \"encoder.layers.encoder_layer_1.mlp.3.weight\", \"encoder.layers.encoder_layer_1.mlp.3.bias\", \"encoder.layers.encoder_layer_2.ln_1.weight\", \"encoder.layers.encoder_layer_2.ln_1.bias\", \"encoder.layers.encoder_layer_2.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_2.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_2.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_2.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_2.ln_2.weight\", \"encoder.layers.encoder_layer_2.ln_2.bias\", \"encoder.layers.encoder_layer_2.mlp.0.weight\", \"encoder.layers.encoder_layer_2.mlp.0.bias\", \"encoder.layers.encoder_layer_2.mlp.3.weight\", \"encoder.layers.encoder_layer_2.mlp.3.bias\", \"encoder.layers.encoder_layer_3.ln_1.weight\", \"encoder.layers.encoder_layer_3.ln_1.bias\", \"encoder.layers.encoder_layer_3.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_3.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_3.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_3.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_3.ln_2.weight\", \"encoder.layers.encoder_layer_3.ln_2.bias\", \"encoder.layers.encoder_layer_3.mlp.0.weight\", \"encoder.layers.encoder_layer_3.mlp.0.bias\", \"encoder.layers.encoder_layer_3.mlp.3.weight\", \"encoder.layers.encoder_layer_3.mlp.3.bias\", \"encoder.layers.encoder_layer_4.ln_1.weight\", \"encoder.layers.encoder_layer_4.ln_1.bias\", \"encoder.layers.encoder_layer_4.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_4.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_4.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_4.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_4.ln_2.weight\", \"encoder.layers.encoder_layer_4.ln_2.bias\", \"encoder.layers.encoder_layer_4.mlp.0.weight\", \"encoder.layers.encoder_layer_4.mlp.0.bias\", \"encoder.layers.encoder_layer_4.mlp.3.weight\", \"encoder.layers.encoder_layer_4.mlp.3.bias\", \"encoder.layers.encoder_layer_5.ln_1.weight\", \"encoder.layers.encoder_layer_5.ln_1.bias\", \"encoder.layers.encoder_layer_5.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_5.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_5.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_5.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_5.ln_2.weight\", \"encoder.layers.encoder_layer_5.ln_2.bias\", \"encoder.layers.encoder_layer_5.mlp.0.weight\", \"encoder.layers.encoder_layer_5.mlp.0.bias\", \"encoder.layers.encoder_layer_5.mlp.3.weight\", \"encoder.layers.encoder_layer_5.mlp.3.bias\", \"encoder.layers.encoder_layer_6.ln_1.weight\", \"encoder.layers.encoder_layer_6.ln_1.bias\", \"encoder.layers.encoder_layer_6.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_6.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_6.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_6.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_6.ln_2.weight\", \"encoder.layers.encoder_layer_6.ln_2.bias\", \"encoder.layers.encoder_layer_6.mlp.0.weight\", \"encoder.layers.encoder_layer_6.mlp.0.bias\", \"encoder.layers.encoder_layer_6.mlp.3.weight\", \"encoder.layers.encoder_layer_6.mlp.3.bias\", \"encoder.layers.encoder_layer_7.ln_1.weight\", \"encoder.layers.encoder_layer_7.ln_1.bias\", \"encoder.layers.encoder_layer_7.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_7.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_7.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_7.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_7.ln_2.weight\", \"encoder.layers.encoder_layer_7.ln_2.bias\", \"encoder.layers.encoder_layer_7.mlp.0.weight\", \"encoder.layers.encoder_layer_7.mlp.0.bias\", \"encoder.layers.encoder_layer_7.mlp.3.weight\", \"encoder.layers.encoder_layer_7.mlp.3.bias\", \"encoder.layers.encoder_layer_8.ln_1.weight\", \"encoder.layers.encoder_layer_8.ln_1.bias\", \"encoder.layers.encoder_layer_8.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_8.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_8.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_8.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_8.ln_2.weight\", \"encoder.layers.encoder_layer_8.ln_2.bias\", \"encoder.layers.encoder_layer_8.mlp.0.weight\", \"encoder.layers.encoder_layer_8.mlp.0.bias\", \"encoder.layers.encoder_layer_8.mlp.3.weight\", \"encoder.layers.encoder_layer_8.mlp.3.bias\", \"encoder.layers.encoder_layer_9.ln_1.weight\", \"encoder.layers.encoder_layer_9.ln_1.bias\", \"encoder.layers.encoder_layer_9.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_9.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_9.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_9.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_9.ln_2.weight\", \"encoder.layers.encoder_layer_9.ln_2.bias\", \"encoder.layers.encoder_layer_9.mlp.0.weight\", \"encoder.layers.encoder_layer_9.mlp.0.bias\", \"encoder.layers.encoder_layer_9.mlp.3.weight\", \"encoder.layers.encoder_layer_9.mlp.3.bias\", \"encoder.layers.encoder_layer_10.ln_1.weight\", \"encoder.layers.encoder_layer_10.ln_1.bias\", \"encoder.layers.encoder_layer_10.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_10.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_10.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_10.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_10.ln_2.weight\", \"encoder.layers.encoder_layer_10.ln_2.bias\", \"encoder.layers.encoder_layer_10.mlp.0.weight\", \"encoder.layers.encoder_layer_10.mlp.0.bias\", \"encoder.layers.encoder_layer_10.mlp.3.weight\", \"encoder.layers.encoder_layer_10.mlp.3.bias\", \"encoder.layers.encoder_layer_11.ln_1.weight\", \"encoder.layers.encoder_layer_11.ln_1.bias\", \"encoder.layers.encoder_layer_11.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_11.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_11.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_11.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_11.ln_2.weight\", \"encoder.layers.encoder_layer_11.ln_2.bias\", \"encoder.layers.encoder_layer_11.mlp.0.weight\", \"encoder.layers.encoder_layer_11.mlp.0.bias\", \"encoder.layers.encoder_layer_11.mlp.3.weight\", \"encoder.layers.encoder_layer_11.mlp.3.bias\", \"encoder.ln.weight\", \"encoder.ln.bias\", \"heads.head.weight\", \"heads.head.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"best_val_acc\", \"history\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-69e6afe6ad07>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the saved weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model_checkpoint.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"class_token\", \"conv_proj.weight\", \"conv_proj.bias\", \"encoder.pos_embedding\", \"encoder.layers.encoder_layer_0.ln_1.weight\", \"encoder.layers.encoder_layer_0.ln_1.bias\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_0.ln_2.weight\", \"encoder.layers.encoder_layer_0.ln_2.bias\", \"encoder.layers.encoder_layer_0.mlp.0.weight\", \"encoder.layers.encoder_layer_0.mlp.0.bias\", \"encoder.layers.encoder_layer_0.mlp.3.weight\", \"encoder.layers.encoder_layer_0.mlp.3.bias\", \"encoder.layers.encoder_layer_1.ln_1.weight\", \"encoder.layers.encoder_layer_1.ln_1.bias\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_1.ln_2.weight\", \"encoder.layers.encoder_layer_1.ln_2.bias\", \"encoder.layers.encoder_layer_1.mlp.0.weight\", \"encoder.layers.encoder_layer_1.mlp.0.bias\", \"encoder.layers.encoder_layer_1.mlp.3.weight\", \"encoder.layers.encoder_layer_1.mlp.3.bias\", \"encoder.layers.encoder_layer_2.ln_1.weight\", \"encoder.layers.encoder_layer_2.ln_1.bias\", \"encoder.layers.encoder_layer_2.self_attention.in_proj_weight\"...\n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"best_val_acc\", \"history\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire checkpoint\n",
        "checkpoint = torch.load('best_model_checkpoint.pth', map_location='cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pU0v6AmQJPx",
        "outputId": "a9fd8d34-978f-480d-abe4-a15124a8115d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d5ac36310ed3>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('best_model_checkpoint.pth', map_location='cpu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the model's state_dict from the checkpoint\n",
        "state_dict = checkpoint['model_state_dict']"
      ],
      "metadata": {
        "id": "KL40_udRQr3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the state_dict into your model\n",
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3ahbYOrQtTV",
        "outputId": "4c72be77-23c9-4f48-a13e-3177e8fddf49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOu3ZA_LQu5Y",
        "outputId": "c7658ee7-0f95-4437-c9ff-e658441ae205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,  # the model instance\n",
        "    {nn.Linear},  # layers to quantize\n",
        "    dtype=torch.qint8  # data type for quantized weights\n",
        ")"
      ],
      "metadata": {
        "id": "_1vYD8nrQwA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnFNtgl_j_Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def print_size_of_model(model, label=''):\n",
        "    torch.save(model.state_dict(), 'temp.p')\n",
        "    size_mb = os.path.getsize('temp.p') / 1e6\n",
        "    print(f'{label} Model Size: {size_mb:.2f} MB')\n",
        "    os.remove('temp.p')\n",
        "    return size_mb\n",
        "\n",
        "# Original model size\n",
        "print_size_of_model(model, 'Original')\n",
        "\n",
        "# Quantized model size\n",
        "print_size_of_model(quantized_model, 'Quantized')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PCItOVeRKCT",
        "outputId": "cc2ede1a-eeda-4e45-efdd-78eaf0318fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Size: 343.52 MB\n",
            "Quantized Model Size: 173.46 MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "173.462068"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Ensure your validation DataLoader is set up\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "quantized_model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to('cpu')  # Quantized model is on CPU\n",
        "        outputs = quantized_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Quantized Model Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy6G572PRMDE",
        "outputId": "2527fd92-f124-40ab-f28b-f8102686c771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Model Accuracy: 0.9654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, data_loader):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to('cpu')\n",
        "            outputs = model(inputs)\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    return total_time\n",
        "\n",
        "# Measure time for original model\n",
        "original_time = measure_inference_time(model, val_loader)\n",
        "print(f'Original Model Inference Time: {original_time:.2f} seconds')\n",
        "\n",
        "# Measure time for quantized model\n",
        "quantized_time = measure_inference_time(quantized_model, val_loader)\n",
        "print(f'Quantized Model Inference Time: {quantized_time:.2f} seconds')\n",
        "\n",
        "# Calculate speed-up\n",
        "speed_up = original_time / quantized_time\n",
        "print(f'Speed-up: {speed_up:.2f}x')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR44D44cRRyd",
        "outputId": "74f98192-14ec-4635-dabd-6fda3c2affee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Inference Time: 1433.81 seconds\n",
            "Quantized Model Inference Time: 1230.51 seconds\n",
            "Speed-up: 1.17x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "normal_model_save_path = '/content/drive/MyDrive/best_model_checkpoint.pth'"
      ],
      "metadata": {
        "id": "IXW3a_a8WCnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), normal_model_save_path)\n",
        "print(f\"Normal model saved to {normal_model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wLk9QvpXepy",
        "outputId": "46e68c54-c464-4ca5-c3c2-f40e31302b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal model saved to /content/drive/MyDrive/best_model_checkpoint.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model_save_path = '/content/drive/MyDrive/quantized_model.pth'"
      ],
      "metadata": {
        "id": "Mq0H_BiZXkZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model, quantized_model_save_path)\n",
        "print(f\"Quantized model saved to {quantized_model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-JaopBEXk3E",
        "outputId": "49711678-f0c7-4e9e-b4e9-b09b944a8bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model saved to /content/drive/MyDrive/quantized_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RgCosZoatDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}